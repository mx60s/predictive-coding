{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d9dd778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torchvision.io import write_video\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "\n",
    "from predictive_coding.models.models import Autoencoder\n",
    "from predictive_coding.dataset import EnvironmentDataset, collate_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5afbae",
   "metadata": {},
   "source": [
    "# Autoencoding\n",
    "\n",
    "In this Google Colab notebook, we apply a pre-trained autoencoding neural network to a dataset containing observations from an agent navigating the Minecraft environment. First, we load the autoencoding neural network. Next, we import the validation dataset that captures episodes of an agent moving through various terrains in Minecraft. Our goal is to utilize the network's decoder to generate latent vectors from the data. These vectors provide a condensed representation of the agent's visual information within the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8682e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'\n",
    "\n",
    "# Initialize the autoencoding architecture\n",
    "model = Autoencoder(in_channels=3, out_channels=3, layers=[2, 2, 2, 2])\n",
    "model = model.to(device)\n",
    "\n",
    "ckpt = torch.load('./experiments/autoencoder/best.ckpt', map_location=device)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecdd5ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8263"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environment observations\n",
    "dataset = EnvironmentDataset(Path(\"../datasets/val-dataset\"))\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e7f979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d0047488904bdb963c4de8c3deed74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the latent vectors from the autoencoding neural network\n",
    "latents = []\n",
    "positions = []\n",
    "for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
    "    images, actions, states = batch\n",
    "    B, L, C, H, W = images.shape\n",
    "    images = images.to(device).reshape(B*L, C, H, W)\n",
    "    states = states.reshape(B*L, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(images)\n",
    "        codes = model.decoder.get_codes(features)\n",
    "        latents.append(codes[1].cpu())\n",
    "        positions.append(states)\n",
    "\n",
    "latents = torch.cat(latents, dim=0).cpu().numpy()\n",
    "positions = torch.cat(positions, dim=0).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6483ed1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[3, 64, 64]}, size=[64, 64]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtidx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m image \u001b[38;5;241m=\u001b[39m Normalize([\u001b[38;5;241m121.6697\u001b[39m, \u001b[38;5;241m149.3242\u001b[39m, \u001b[38;5;241m154.9510\u001b[39m], [\u001b[38;5;241m40.7521\u001b[39m,  \u001b[38;5;241m47.7267\u001b[39m, \u001b[38;5;241m103.2739\u001b[39m])(ToTensor()(image))\n\u001b[0;32m---> 17\u001b[0m \u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtidx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m image\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[3, 64, 64]}, size=[64, 64]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "folds = glob('../datasets/pc-images/*')\n",
    "images = torch.empty(len(folds), 3, 64, 64)\n",
    "positions = torch.zeros(len(folds), 2)\n",
    "for idx, fold in enumerate(folds):\n",
    "    x, y = re.findall('../datasets/pc-images/(.*)_(.*)', fold)[0]\n",
    "    positions[idx] = torch.from_numpy(np.load(f'{fold}/states.npy'))[-1, :2]\n",
    "    \n",
    "    for tidx in range(10):\n",
    "        if not os.path.exists(f'{fold}/{tidx}.png'):\n",
    "            images[idx, tidx] = torch.zeros(3, 64, 64)\n",
    "            print(tidx, x, y)\n",
    "            continue\n",
    "\n",
    "        image = Image.open(f'{fold}/{tidx}.png')\n",
    "        image = Normalize([121.6697, 149.3242, 154.9510], [40.7521,  47.7267, 103.2739])(ToTensor()(image))\n",
    "    \n",
    "        images[idx, tidx] = image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = []\n",
    "\n",
    "model = model.to('cuda:1')\n",
    "\n",
    "bsz = 100\n",
    "for idx in range(len(images) // bsz + 1):\n",
    "    batch = images[bsz*idx:bsz*(idx+1)].to('cuda:1')\n",
    "    B, L, C, H, W = batch.shape\n",
    "    batch = batch.to(\"cuda:1\").reshape(B*L, C, H, W)\n",
    "    act = actions[bsz*idx:bsz*(idx+1)].to(\"cuda:1\").reshape(B*L, -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(batch)\n",
    "        codes = model.decoder.get_codes(features)\n",
    "        latents.append(codes[1].cpu().reshape(B, L, -1, 8, 8))\n",
    "        \n",
    "latents = torch.cat(latents, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e13b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(128, 256, 3, padding=1),\n",
    "    nn.MaxPool2d(2),\n",
    "    Lambda(lambda x: x.reshape(-1, 256*4*4)),\n",
    "    nn.Linear(64*8*8, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 2)\n",
    ")\n",
    "\n",
    "net = net.to(\"cuda:0\")\n",
    "    \n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573716f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "for epoch in range(10000):\n",
    "    batch_idx = np.arange(0, len(latents))\n",
    "    np.random.shuffle(batch_idx)\n",
    "    batch_idx = batch_idx[:len(batch_idx) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for it, idx in enumerate(batch_idx):\n",
    "        optimizer.zero_grad()\n",
    "        batch = latents[idx, -1].to(\"cuda:0\")\n",
    "        pos = positions[idx, :2].to(\"cuda:0\") / 30\n",
    "        pred = net(batch)\n",
    "        loss = F.mse_loss(pred, pos)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if it % 100:\n",
    "            with torch.no_grad():\n",
    "                pred = net(latents[:1000, -1].to(\"cuda:0\")).cpu() * 30\n",
    "                print(F.mse_loss(pred, positions[:1000, :2]), end='\\r')\n",
    "    scheduler.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch = latents[:, -1].to(\"cuda:0\")\n",
    "    ae_pos = positions[:, :2].cpu() / 30\n",
    "    ae_pred = net(batch).cpu()\n",
    "    ae = torch.linalg.norm(ae_pred * 30 - positions[:, :2], dim=1).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da881d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "null = np.linalg.norm(np.random.randint(30, size=(2, 1000)) - np.random.randint(30, size=(2, 1000)), axis=0)\n",
    "grid = np.stack(np.mgrid[-20:20,-30:35])\n",
    "rand = np.linalg.norm(np.random.randn(*grid.shape), axis=0).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dbb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "for idx in range(len(ae_pos)):\n",
    "    plt.plot([-ae_pred[idx, 1], -ae_pos[idx, 1]], [ae_pred[idx, 0], ae_pos[idx, 0]], alpha=0.5, color=plt.cm.Greys(0.3), zorder=-1)\n",
    "act = plt.scatter(-ae_pos[:, 1], ae_pos[:, 0], s=1*scale, c=plt.cm.Greys(0.6), label='Actual')\n",
    "p = plt.scatter(-ae_pred[:, 1], ae_pred[:, 0], s=1*scale, c=plt.cm.Blues(0.75), label='Predicted')\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"x (lattice units)\")\n",
    "plt.ylabel(\"y (lattice units)\")\n",
    "plt.legend([Line2D([0], [0], marker='o', color=plt.cm.Greys(0.6), markersize=2, linestyle=\"None\"), \n",
    "            Line2D([0], [0], marker='o', color=plt.cm.Blues(0.75), markersize=2, linestyle=\"None\"),\n",
    "            Line2D([0], [0], color=plt.cm.Greys(0.5), lw=2.0)], ['Actual', 'Predicted', 'Error'], \n",
    "           loc='upper right', bbox_to_anchor=(1.1, 1.0), prop=font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14801c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "sns.histplot(pc, kde=True, stat=\"density\", label=\"Autoenoding\")\n",
    "sns.kdeplot(null, fill=True, label=\"Random Pairs\", color=plt.cm.Dark2(0))\n",
    "sns.kdeplot(rand, fill=True, label=\"Noise Model ($\\sigma=1$)\", color=plt.cm.Dark2(1))\n",
    "plt.xlabel(\"Error ($\\Vert x - \\hat{x}(z) \\Vert_{\\ell_2}$) (lattice units)\")\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
